#!/usr/bin/env python3
"""
Google Translate Safe Version
C·∫£i ti·∫øn v·ªõi advanced rate limiting, caching, v√† error handling
"""
import time
import random
import hashlib
import json
import os
from pathlib import Path
from typing import List, Optional, Dict, Any
import requests
import threading
from datetime import datetime, timedelta

class SafeGoogleTranslateAPI:
    def __init__(self, cache_dir="translation_cache"):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        self.base_url = "https://translate.googleapis.com/translate_a/single"
        
        # Advanced rate limiting
        self.min_delay = 1.5  # TƒÉng delay t·ªëi thi·ªÉu
        self.max_delay = 3.0  # Delay t·ªëi ƒëa
        self.current_delay = self.min_delay
        self.consecutive_errors = 0
        self.max_chunk_size = 3000  # Gi·∫£m chunk size ƒë·ªÉ an to√†n h∆°n
        
        # Request tracking
        self.requests_this_minute = 0
        self.requests_this_hour = 0
        self.last_minute_reset = datetime.now()
        self.last_hour_reset = datetime.now()
        self.max_requests_per_minute = 25  # Conservative limit
        self.max_requests_per_hour = 1000
        
        # Caching system
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.cache = {}
        self.load_cache()
        
        # Thread safety
        self.lock = threading.Lock()
        
        # Statistics
        self.stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'errors': 0,
            'blocked_periods': 0
        }
    
    def load_cache(self):
        """T·∫£i cache t·ª´ file"""
        cache_file = self.cache_dir / "translation_cache.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    self.cache = json.load(f)
                print(f"üì¶ Loaded {len(self.cache)} cached translations")
            except Exception as e:
                print(f"‚ö†Ô∏è Could not load cache: {e}")
                self.cache = {}
    
    def save_cache(self):
        """L∆∞u cache v√†o file"""
        cache_file = self.cache_dir / "translation_cache.json"
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"‚ö†Ô∏è Could not save cache: {e}")
    
    def get_cache_key(self, text, target_lang, source_lang='en'):
        """T·∫°o cache key t·ª´ text v√† language"""
        combined = f"{source_lang}->{target_lang}:{text}"
        return hashlib.md5(combined.encode('utf-8')).hexdigest()
    
    def get_cached_translation(self, text, target_lang, source_lang='en'):
        """L·∫•y b·∫£n d·ªãch t·ª´ cache n·∫øu c√≥"""
        cache_key = self.get_cache_key(text, target_lang, source_lang)
        return self.cache.get(cache_key)
    
    def cache_translation(self, text, translation, target_lang, source_lang='en'):
        """L∆∞u b·∫£n d·ªãch v√†o cache"""
        cache_key = self.get_cache_key(text, target_lang, source_lang)
        self.cache[cache_key] = translation
        
        # ƒê·ªãnh k·ª≥ l∆∞u cache (m·ªói 50 translations)
        if len(self.cache) % 50 == 0:
            self.save_cache()
    
    def check_rate_limits(self):
        """Ki·ªÉm tra v√† reset rate limits"""
        now = datetime.now()
        
        # Reset minute counter
        if now - self.last_minute_reset > timedelta(minutes=1):
            self.requests_this_minute = 0
            self.last_minute_reset = now
        
        # Reset hour counter
        if now - self.last_hour_reset > timedelta(hours=1):
            self.requests_this_hour = 0
            self.last_hour_reset = now
        
        # Check limits
        if self.requests_this_minute >= self.max_requests_per_minute:
            wait_time = 60 - (now - self.last_minute_reset).seconds
            print(f"‚è≥ Rate limit reached, waiting {wait_time}s...")
            time.sleep(wait_time + 1)
            self.check_rate_limits()  # Recheck after waiting
        
        if self.requests_this_hour >= self.max_requests_per_hour:
            wait_time = 3600 - (now - self.last_hour_reset).seconds
            print(f"‚è≥ Hourly limit reached, waiting {wait_time/60:.1f} minutes...")
            self.stats['blocked_periods'] += 1
            time.sleep(wait_time + 1)
            self.check_rate_limits()
    
    def adaptive_delay(self, success=True):
        """Adaptive delay based on success/failure"""
        if success:
            # Gi·∫£m delay n·∫øu th√†nh c√¥ng
            self.current_delay = max(self.min_delay, self.current_delay * 0.9)
            self.consecutive_errors = 0
        else:
            # TƒÉng delay n·∫øu l·ªói (exponential backoff)
            self.consecutive_errors += 1
            backoff_factor = min(2.0 ** self.consecutive_errors, 8.0)  # Cap at 8x
            self.current_delay = min(self.max_delay * backoff_factor, 30.0)  # Cap at 30s
        
        # Random jitter ƒë·ªÉ tr√°nh pattern detection
        jitter = random.uniform(0.8, 1.2)
        actual_delay = self.current_delay * jitter
        
        print(f"‚è±Ô∏è Waiting {actual_delay:.1f}s (errors: {self.consecutive_errors})")
        time.sleep(actual_delay)
    
    def get_language_code(self, lang_code):
        """Chuy·ªÉn ƒë·ªïi language code"""
        lang_map = {
            'VI': 'vi', 'JA': 'ja', 'EN': 'en', 'ZH': 'zh',
            'FR': 'fr', 'DE': 'de', 'ES': 'es'
        }
        return lang_map.get(lang_code.upper(), lang_code.lower())
    
    def split_text_into_chunks(self, texts, max_size=None):
        """Chia nh·ªè texts v·ªõi size adaptive"""
        if max_size is None:
            max_size = self.max_chunk_size
        
        chunks = []
        current_chunk = []
        current_size = 0
        
        for text in texts:
            text_size = len(text.encode('utf-8'))
            
            # N·∫øu text qu√° l·ªõn, chia nh·ªè
            if text_size > max_size:
                if current_chunk:
                    chunks.append(current_chunk)
                    current_chunk = []
                    current_size = 0
                
                # Chia text l·ªõn th√†nh t·ª´ng t·ª´
                words = text.split()
                temp_chunk = []
                temp_size = 0
                
                for word in words:
                    word_size = len(word.encode('utf-8')) + 1
                    if temp_size + word_size > max_size and temp_chunk:
                        chunks.append([' '.join(temp_chunk)])
                        temp_chunk = [word]
                        temp_size = word_size
                    else:
                        temp_chunk.append(word)
                        temp_size += word_size
                
                if temp_chunk:
                    chunks.append([' '.join(temp_chunk)])
            else:
                # Ki·ªÉm tra xem c√≥ th·ªÉ th√™m v√†o chunk hi·ªán t·∫°i kh√¥ng
                if current_size + text_size > max_size and current_chunk:
                    chunks.append(current_chunk)
                    current_chunk = [text]
                    current_size = text_size
                else:
                    current_chunk.append(text)
                    current_size += text_size
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def translate_chunk_with_cache(self, texts, target_lang, source_lang='en'):
        """D·ªãch chunk v·ªõi cache support"""
        if not texts:
            return []
        
        # Ki·ªÉm tra cache cho t·ª´ng text
        cached_results = []
        uncached_texts = []
        uncached_indices = []
        
        for i, text in enumerate(texts):
            cached = self.get_cached_translation(text, target_lang, source_lang)
            if cached:
                cached_results.append((i, cached))
                self.stats['cache_hits'] += 1
            else:
                uncached_texts.append(text)
                uncached_indices.append(i)
                self.stats['cache_misses'] += 1
        
        # N·∫øu t·∫•t c·∫£ ƒë√£ c√≥ trong cache
        if not uncached_texts:
            result = [''] * len(texts)
            for i, translation in cached_results:
                result[i] = translation
            print(f"üíæ All {len(texts)} texts from cache")
            return result
        
        # D·ªãch c√°c text ch∆∞a c√≥ trong cache
        print(f"üíæ {len(cached_results)} from cache, {len(uncached_texts)} need translation")
        
        try:
            with self.lock:
                self.check_rate_limits()
                self.stats['total_requests'] += 1
                self.requests_this_minute += 1
                self.requests_this_hour += 1
            
            translated_uncached = self.translate_chunk_direct(uncached_texts, target_lang, source_lang)
            
            # Cache c√°c k·∫øt qu·∫£ m·ªõi
            for text, translation in zip(uncached_texts, translated_uncached):
                self.cache_translation(text, translation, target_lang, source_lang)
            
            # K·∫øt h·ª£p k·∫øt qu·∫£
            result = [''] * len(texts)
            for i, translation in cached_results:
                result[i] = translation
            
            for i, translation in zip(uncached_indices, translated_uncached):
                result[i] = translation
            
            self.adaptive_delay(success=True)
            return result
            
        except Exception as e:
            print(f"‚ùå Translation error: {e}")
            self.stats['errors'] += 1
            self.adaptive_delay(success=False)
            return texts  # Tr·∫£ v·ªÅ text g·ªëc n·∫øu l·ªói
    
    def translate_chunk_direct(self, texts, target_lang, source_lang='en'):
        """D·ªãch chunk tr·ª±c ti·∫øp (kh√¥ng cache)"""
        try:
            combined_text = '\n'.join(texts)
            
            params = {
                'client': 'gtx',
                'sl': source_lang,
                'tl': target_lang,
                'dt': 't',
                'q': combined_text
            }
            
            response = self.session.get(self.base_url, params=params, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                
                if result and len(result) > 0 and result[0]:
                    translated_parts = []
                    for part in result[0]:
                        if part and len(part) > 0:
                            translated_parts.append(part[0])
                    
                    translated_text = ''.join(translated_parts)
                    translated_lines = translated_text.split('\n')
                    
                    if len(translated_lines) == len(texts):
                        return translated_lines
                    else:
                        # Fallback: chia ƒë·ªÅu
                        words_per_text = len(translated_text.split()) // len(texts)
                        words = translated_text.split()
                        result_texts = []
                        for i in range(len(texts)):
                            start_idx = i * words_per_text
                            end_idx = (i + 1) * words_per_text if i < len(texts) - 1 else len(words)
                            result_texts.append(' '.join(words[start_idx:end_idx]))
                        return result_texts
                else:
                    raise Exception("Empty response from Google Translate")
            else:
                raise Exception(f"HTTP {response.status_code}: {response.text}")
                
        except Exception as e:
            raise e
    
    def translate_texts(self, texts, target_lang, source_lang='en', progress_callback=None):
        """Main translation method v·ªõi all improvements"""
        if not texts:
            return []
        
        target_lang = self.get_language_code(target_lang)
        source_lang = self.get_language_code(source_lang)
        
        print(f"üîí Safe Google Translate: {len(texts)} texts {source_lang} -> {target_lang}")
        print(f"üìä Cache: {len(self.cache)} entries, Max RPM: {self.max_requests_per_minute}")
        
        chunks = self.split_text_into_chunks(texts, self.max_chunk_size)
        print(f"üì¶ Split into {len(chunks)} chunks (max size: {self.max_chunk_size})")
        
        all_results = []
        
        for i, chunk in enumerate(chunks):
            try:
                if progress_callback:
                    progress_callback(i, len(chunks), f"Safe translate chunk {i+1}/{len(chunks)}")
                
                chunk_results = self.translate_chunk_with_cache(chunk, target_lang, source_lang)
                all_results.extend(chunk_results)
                
                print(f"‚úÖ Chunk {i+1}/{len(chunks)}: {len(chunk)} texts")
                
            except Exception as e:
                print(f"‚ùå Chunk {i+1} failed: {e}")
                all_results.extend(chunk)  # Fallback to original
        
        # L∆∞u cache cu·ªëi c√πng
        self.save_cache()
        
        # In th·ªëng k√™
        self.print_statistics()
        
        return all_results
    
    def print_statistics(self):
        """In th·ªëng k√™ s·ª≠ d·ª•ng"""
        print(f"\nüìà TRANSLATION STATISTICS:")
        print(f"‚Ä¢ Total requests: {self.stats['total_requests']}")
        print(f"‚Ä¢ Cache hits: {self.stats['cache_hits']}")
        print(f"‚Ä¢ Cache misses: {self.stats['cache_misses']}")
        hit_rate = self.stats['cache_hits'] / max(self.stats['cache_hits'] + self.stats['cache_misses'], 1)
        print(f"‚Ä¢ Cache hit rate: {hit_rate:.1%}")
        print(f"‚Ä¢ Errors: {self.stats['errors']}")
        print(f"‚Ä¢ Blocked periods: {self.stats['blocked_periods']}")
        print(f"‚Ä¢ Current delay: {self.current_delay:.1f}s")

def test_safe_translation():
    """Test Safe Google Translate"""
    translator = SafeGoogleTranslateAPI()
    
    test_texts = [
        "Iron plate",
        "Copper wire", 
        "Steam engine",
        "Electric furnace",
        "Advanced circuit",
        "Iron plate",  # Duplicate ƒë·ªÉ test cache
        "Assembly machine"
    ]
    
    print("üß™ Testing Safe Google Translate...")
    results = translator.translate_texts(test_texts, 'VI', 'en')
    
    print("\nüìã Results:")
    for orig, trans in zip(test_texts, results):
        print(f"  EN: {orig}")
        print(f"  VI: {trans}")
        print()

if __name__ == "__main__":
    test_safe_translation()
